---
title: "ECPA13"
author: "Peter Edelsbrunner, Fabian Dablander"
date: "`r Sys.Date()`"
output: html_document
---

## Vignette Info
This gives an overview of the simulations we presented at [ECPA13](http://www.ecpa13.com/). They demonstrate that itemfit statistics should not be used so recklessly as they currently are in the literature of scientific thinking. The simulated data is included in this package, since running the simulations from scratch takes some time. However, for clarity we have also included the calls made to produce the simulations (in comments).

## Simulation Parameters

```{r}
source('../R/main.R')
#library('simrasch')

## PARAMETERS
n <- c(100, 300, 1000) # number of participants
items <- c(10, 20, 50) # number of items
corr <- c(.1, .3, .5) # violation of local dependence
discr <- c(.1, .3, .5) # violation of equal item discrimination
```

## Local Dependency

```{r}
#res_dep1 <- main_sim(n, items, sim.dep, .1)
#res_dep3 <- main_sim(n, items, sim.dep, .3)
#res_dep5 <- main_sim(n, items, sim.dep, .5)
```

## Item Discriminability
```{r}
#res_2pl1 <- main_sim(n, items, sim.2pl, .1)
#res_2pl3 <- main_sim(n, items, sim.2pl, .3)
#res_2pl5 <- main_sim(n, items, sim.2pl, .5)
```

## Results
```{r}
load('../data/sim_results.RData')
get_percent <- function(sim) sim$rm_percent
```

Below we look at how the itemfit statistics (infit, standardized p-values) behave under various model misspecifications, and once when the true model is really the Rasch model.

## True model: Rasch

```{r}
# cutoff c(.8, 1.2) is default
err_rasch <- compute_errors(rasch)
rm_rasch <- sapply(err_rasch, get_percent)
rm_pval <- sapply(err_rasch, function(sim) sim$pval)
plot(rm_rasch, type = 'l', col = 'blue', ylim = c(0, .1))
lines(rm_pval, type = 'l', col = 'red')
```

When the Rasch model is the data generating model, itemfit statistics using the cutoff of .8 and 1.2 are reasonably good they item removal suggestions. With increasing sample size it goes to zero. Standardized p-values, on the other hand, do not show this asymptotically correct behavior; p-values are inconsistent.

## Violation: Item discrimination
```{r}
err_2pl <- compute_errors(res_2pl5)
rm_2pl <- sapply(err_2pl, get_percent)
plot(rm_2pl, type = 'l', col = 'blue', ylim = c(0, .25))
```

When the item discrimination is violated, i.e. not equal across items as the Rasch model requires, infit statistics require us to remove approximately 20 percent of the items in small sample size scenarios, and about 12 percent in large sample sizes.

Intriguingly, one can not really conclude anything at this point. One reasoning is that the items are no good, say because of their formulations, and thus need to be removed; or the Rasch model simply is inadequate. The different conclusions nicely embody the difference between the two "Rasch school", one applied, one theoretical. The fundamental problem is that one cannot make theoretical claims based on such "applied" model fitting procedures.

## Violation: Local Dependence
```{r}
err_dep <- compute_errors(res_dep5)
rm_dep <- sapply(err_dep, get_percent)
plot(rm_dep, type = 'l', col = 'blue', ylim = c(0, .1))
```


When items are pairwisely, but unsystematically correlated (that is, there is no systematic multidimensionality, say if we would have two factors), the infit statistic is absolutely insensitive. Even though the Rasch model is clearly inadequate - local independence is not given! - infit statistics do not capture this.


## Example
Simulate a four-dimensional Model (as common in the literature on scientific reasoning)

```{r}
library('mirt')


items <- 60
weights <- rbind(c(.5, 0, 0, 0), c(0, .5, 0, 0), c(0, 0, .5, 0), c(0, 0, 0, .5))
wmat <- apply(replicate(items / 4, weights), 2, rbind) # four factors
Sigma <- matrix(c(1, 0.5, 0.4, 0.3, 0.5, 1, 0.6, 0.3, 0.4, 0.6, 1, 0.2, 0.3, 0.3, 0.2, 1), 4)

s <- sim.xdim(1000, 60, Sigma = Sigma, weightmat = wmat)
model <- 'f1=1, 5, 9, 13, 17, 21, 25, 29, 33, 37, 41, 45, 49, 53, 57
          f2=2, 6, 10, 14, 18, 22, 26, 30, 34, 38, 42, 46, 50, 54, 58
          f3=3, 7, 11, 15, 19, 23, 27, 31, 35, 39, 43, 47, 51, 55, 59
          f4=4, 8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48, 52, 56, 60
          cov=f1*f2*f3*f4'

mrasch <- RM(s)
fitm <- mirt(s, 1, itemtype = 'Rasch')
fitpl <- mirt(s, 1, itemtype = '2PL')
```

The reliability of the pseudo one-dimension is:
```{r}
fscores(fitm, returnER = TRUE)
```

whereas each subscale (all items that load of exactly one factor) is lower:
```{r}
f1 <- c(1, 5, 9, 13, 17, 21, 25, 29, 33, 37, 41, 45, 49, 53, 57)
f2 <- c(2, 6, 10, 14, 18, 22, 26, 30, 34, 38, 42, 46, 50, 54, 58)
fit_factor1 <- mirt(s[, f1], 1, temtype = 'Rasch')
fit_factor2 <- mirt(s[, f2], 1, temtype = 'Rasch')

fscores(fit_factor1, returnER = TRUE)
fscores(fit_factor2, returnER = TRUE)
```

But these factors measure something distinct! By aggregating over the factors we actually loose information.


## itemfits in four-dimensional space

We see that itemfit statistics **completely miss the point**.
```{r}
pp <- person.parameter(mrasch)
eRm::itemfit(pp)
```

A non-parametric Martin LÃ¶ef test, for example, does reject the one-dimensional Rasch model, splitting for each subscales:

```{r}
start <- Sys.time()
#res <- NPtest(s, n = 50, method = 'MLoef', splitcr = rep(1:4, times = 15))
end <- Sys.time()
```
